#!/usr/bin/env python3
"""
Script de Treino Offline - Modelo de DeteÃ§Ã£o de Regime de Mercado

Este script treina um modelo KMeans para identificar 2 regimes de mercado:
- Regime 0 (Sideways): Baixa volatilidade e ADX
- Regime 1 (Trending): Alta volatilidade e ADX

O modelo treinado Ã© salvo em disco e pode ser carregado pelo bot para
adaptar suas estratÃ©gias em tempo real.
"""

import pandas as pd
import joblib
from pathlib import Path
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import numpy as np

# Importar funÃ§Ã£o de feature engineering do nosso mÃ³dulo
from src.core.feature_engineering import calcular_features


def carregar_dados_historicos(caminho_csv: str) -> pd.DataFrame:
    """
    Carrega dados histÃ³ricos OHLCV de um arquivo CSV.
    
    Args:
        caminho_csv: Caminho para o arquivo CSV com dados histÃ³ricos
        
    Returns:
        DataFrame com dados OHLCV
    """
    print(f"ğŸ“‚ Carregando dados histÃ³ricos de: {caminho_csv}")
    
    df = pd.read_csv(caminho_csv)
    
    # Normalizar nomes de colunas para minÃºsculas
    df.columns = df.columns.str.lower()
    
    # Verificar colunas necessÃ¡rias
    colunas_necessarias = ['open', 'high', 'low', 'close', 'volume']
    for col in colunas_necessarias:
        if col not in df.columns:
            raise ValueError(f"Coluna '{col}' nÃ£o encontrada no CSV")
    
    print(f"âœ… {len(df)} velas carregadas")
    print(f"   PerÃ­odo: {df.index[0] if 'timestamp' in df.columns else 'N/A'} atÃ© {df.index[-1] if 'timestamp' in df.columns else 'N/A'}")
    
    return df


def preparar_features_para_clustering(df: pd.DataFrame) -> tuple[pd.DataFrame, np.ndarray]:
    """
    Prepara features para treino do modelo de clustering.
    
    Args:
        df: DataFrame com features calculadas
        
    Returns:
        Tupla (df_original, features_normalizadas)
    """
    # Selecionar features relevantes para deteÃ§Ã£o de regime
    features_regime = ['volatilidade', 'adx']
    
    # Verificar se features existem
    for feature in features_regime:
        if feature not in df.columns:
            raise ValueError(f"Feature '{feature}' nÃ£o encontrada. Execute calcular_features() primeiro.")
    
    # Extrair features e remover NaNs
    X = df[features_regime].values
    
    # Verificar NaNs
    if np.isnan(X).any():
        print("âš ï¸  Aviso: NaNs encontrados nas features. Removendo...")
        mask = ~np.isnan(X).any(axis=1)
        X = X[mask]
        df = df[mask]
    
    print(f"\nğŸ“Š Features selecionadas para clustering:")
    print(f"   - volatilidade")
    print(f"   - adx")
    print(f"   Total de amostras: {len(X)}")
    
    # Normalizar features (importante para KMeans)
    scaler = StandardScaler()
    X_normalized = scaler.fit_transform(X)
    
    print(f"\nğŸ”„ Features normalizadas (StandardScaler)")
    print(f"   MÃ©dia: {X_normalized.mean(axis=0)}")
    print(f"   Desvio padrÃ£o: {X_normalized.std(axis=0)}")
    
    return df, X_normalized, scaler


def treinar_modelo_kmeans(X: np.ndarray, n_clusters: int = 2, random_state: int = 42) -> KMeans:
    """
    Treina modelo KMeans para deteÃ§Ã£o de regime.
    
    Args:
        X: Array de features normalizadas
        n_clusters: NÃºmero de clusters (regimes)
        random_state: Seed para reprodutibilidade
        
    Returns:
        Modelo KMeans treinado
    """
    print(f"\nğŸ¤– Treinando modelo KMeans...")
    print(f"   n_clusters: {n_clusters}")
    print(f"   random_state: {random_state}")
    
    modelo = KMeans(
        n_clusters=n_clusters,
        random_state=random_state,
        n_init=10,  # NÃºmero de inicializaÃ§Ãµes
        max_iter=300  # MÃ¡ximo de iteraÃ§Ãµes
    )
    
    modelo.fit(X)
    
    print(f"âœ… Modelo treinado com sucesso!")
    print(f"   InÃ©rcia: {modelo.inertia_:.2f}")
    print(f"   IteraÃ§Ãµes: {modelo.n_iter_}")
    
    return modelo


def analisar_clusters(modelo: KMeans, scaler: StandardScaler, df: pd.DataFrame) -> None:
    """
    Analisa os centros dos clusters para identificar regimes.
    
    Args:
        modelo: Modelo KMeans treinado
        scaler: Scaler usado na normalizaÃ§Ã£o
        df: DataFrame original com features
    """
    print(f"\n" + "=" * 70)
    print("ANÃLISE DOS CLUSTERS (REGIMES DE MERCADO)")
    print("=" * 70)
    
    # Desnormalizar centros dos clusters
    centros_normalizados = modelo.cluster_centers_
    centros_originais = scaler.inverse_transform(centros_normalizados)
    
    # Predizer labels para todo o dataset
    features = df[['volatilidade', 'adx']].values
    features_normalizadas = scaler.transform(features)
    labels = modelo.predict(features_normalizadas)
    
    # Adicionar labels ao dataframe
    df['regime'] = labels
    
    # Analisar cada cluster
    for i in range(len(centros_originais)):
        print(f"\n{'â”€' * 70}")
        print(f"CLUSTER {i}")
        print(f"{'â”€' * 70}")
        
        # Centro do cluster
        volatilidade_centro = centros_originais[i][0]
        adx_centro = centros_originais[i][1]
        
        print(f"Centro do cluster:")
        print(f"  Volatilidade: {volatilidade_centro:.4f}")
        print(f"  ADX:          {adx_centro:.2f}")
        
        # EstatÃ­sticas das amostras deste cluster
        cluster_samples = df[df['regime'] == i]
        n_samples = len(cluster_samples)
        percentual = (n_samples / len(df)) * 100
        
        print(f"\nEstatÃ­sticas ({n_samples} amostras, {percentual:.1f}% do total):")
        print(f"  Volatilidade: mÃ©dia={cluster_samples['volatilidade'].mean():.4f}, "
              f"std={cluster_samples['volatilidade'].std():.4f}")
        print(f"  ADX:          mÃ©dia={cluster_samples['adx'].mean():.2f}, "
              f"std={cluster_samples['adx'].std():.2f}")
        
        # Identificar tipo de regime
        if adx_centro > 25 or volatilidade_centro > df['volatilidade'].median():
            regime_tipo = "ğŸ”¥ TRENDING (Alta volatilidade/tendÃªncia)"
        else:
            regime_tipo = "ğŸ“Š SIDEWAYS (Baixa volatilidade/consolidaÃ§Ã£o)"
        
        print(f"\nInterpretaÃ§Ã£o: {regime_tipo}")
    
    print(f"\n{'â•' * 70}")
    
    # DistribuiÃ§Ã£o dos regimes
    print(f"\nğŸ“ˆ DistribuiÃ§Ã£o dos Regimes:")
    print(df['regime'].value_counts().sort_index())
    print(f"\nProporÃ§Ã£o:")
    print(df['regime'].value_counts(normalize=True).sort_index())


def salvar_modelo_e_scaler(modelo: KMeans, scaler: StandardScaler, 
                           caminho_modelo: str = 'dados/modelo_regime_kmeans.pkl',
                           caminho_scaler: str = 'dados/scaler_regime.pkl') -> None:
    """
    Salva o modelo treinado e o scaler em disco.
    
    Args:
        modelo: Modelo KMeans treinado
        scaler: StandardScaler usado
        caminho_modelo: Caminho para salvar o modelo
        caminho_scaler: Caminho para salvar o scaler
    """
    # Criar diretÃ³rio se nÃ£o existir
    Path(caminho_modelo).parent.mkdir(parents=True, exist_ok=True)
    
    # Salvar modelo
    joblib.dump(modelo, caminho_modelo)
    print(f"\nğŸ’¾ Modelo salvo em: {caminho_modelo}")
    
    # Salvar scaler
    joblib.dump(scaler, caminho_scaler)
    print(f"ğŸ’¾ Scaler salvo em: {caminho_scaler}")
    
    # Verificar tamanhos dos arquivos
    tamanho_modelo = Path(caminho_modelo).stat().st_size / 1024  # KB
    tamanho_scaler = Path(caminho_scaler).stat().st_size / 1024  # KB
    
    print(f"\nğŸ“¦ Tamanho dos arquivos:")
    print(f"   Modelo: {tamanho_modelo:.2f} KB")
    print(f"   Scaler: {tamanho_scaler:.2f} KB")


def main():
    """FunÃ§Ã£o principal do script de treino."""
    
    print("=" * 70)
    print("ğŸš€ TREINO DO MODELO DE DETEÃ‡ÃƒO DE REGIME DE MERCADO")
    print("=" * 70)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CONFIGURAÃ‡ÃƒO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    CAMINHO_CSV = 'dados/historico_btcusdt_1h.csv'  # Ajuste conforme necessÃ¡rio
    CAMINHO_MODELO = 'dados/modelo_regime_kmeans.pkl'
    CAMINHO_SCALER = 'dados/scaler_regime.pkl'
    N_CLUSTERS = 2  # Sideways vs Trending
    RANDOM_STATE = 42
    
    try:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 1. CARREGAR DADOS HISTÃ“RICOS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        df = carregar_dados_historicos(CAMINHO_CSV)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 2. CALCULAR FEATURES
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\n{'â”€' * 70}")
        print("CALCULANDO FEATURES TÃ‰CNICAS")
        print(f"{'â”€' * 70}")
        
        df_features = calcular_features(df)
        
        print(f"âœ… Features calculadas")
        print(f"   Shape final: {df_features.shape}")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 3. PREPARAR FEATURES PARA CLUSTERING
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\n{'â”€' * 70}")
        print("PREPARANDO FEATURES PARA CLUSTERING")
        print(f"{'â”€' * 70}")
        
        df_preparado, X_normalized, scaler = preparar_features_para_clustering(df_features)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 4. TREINAR MODELO KMEANS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\n{'â”€' * 70}")
        print("TREINO DO MODELO")
        print(f"{'â”€' * 70}")
        
        modelo = treinar_modelo_kmeans(X_normalized, n_clusters=N_CLUSTERS, random_state=RANDOM_STATE)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 5. ANALISAR CLUSTERS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        analisar_clusters(modelo, scaler, df_preparado)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 6. SALVAR MODELO E SCALER
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\n{'â”€' * 70}")
        print("SALVANDO MODELO")
        print(f"{'â”€' * 70}")
        
        salvar_modelo_e_scaler(modelo, scaler, CAMINHO_MODELO, CAMINHO_SCALER)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # SUCESSO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\n{'â•' * 70}")
        print("âœ… TREINO CONCLUÃDO COM SUCESSO!")
        print(f"{'â•' * 70}")
        
        print(f"\nğŸ“‹ PrÃ³ximos passos:")
        print(f"   1. Verifique os centros dos clusters acima")
        print(f"   2. Identifique qual cluster Ã© 'Trending' e qual Ã© 'Sideways'")
        print(f"   3. Integre o modelo no bot usando joblib.load()")
        print(f"   4. Use modelo.predict() em tempo real para adaptar estratÃ©gias")
        
        print(f"\nğŸ’¡ Exemplo de uso no bot:")
        print(f"   modelo = joblib.load('{CAMINHO_MODELO}')")
        print(f"   scaler = joblib.load('{CAMINHO_SCALER}')")
        print(f"   features = scaler.transform([[volatilidade, adx]])")
        print(f"   regime = modelo.predict(features)[0]  # 0 ou 1")
        
    except FileNotFoundError as e:
        print(f"\nâŒ ERRO: Arquivo nÃ£o encontrado")
        print(f"   {e}")
        print(f"\nğŸ’¡ Certifique-se de que existe um arquivo CSV com dados histÃ³ricos em:")
        print(f"   {CAMINHO_CSV}")
        print(f"\n   O arquivo deve conter colunas: open, high, low, close, volume")
        
    except ValueError as e:
        print(f"\nâŒ ERRO: {e}")
        
    except Exception as e:
        print(f"\nâŒ ERRO INESPERADO: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
